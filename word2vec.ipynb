{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "import jieba\n",
    "jieba.enable_parallel(4)\n",
    "source_path = 'data/train70.zh'\n",
    "target_path = 'data/train70.en'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_list = jieba.cut(source_text)\n",
    "# source_text = \" \".join(seg_list)\n",
    "f = open('train_fenci.zh', \"w+\")\n",
    "f.truncate()\n",
    "for sentence in source_text.split('\\n'):\n",
    "    if sentence != '':\n",
    "        seg = jieba.cut(sentence)\n",
    "        temp = \" \".join(seg) + '\\n'\n",
    "        f.write(temp)\n",
    "    \n",
    "f.close()\n",
    "source_text = helper.load_data('train_fenci.zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 180148\n",
      "Number of sentences: 700001\n",
      "Number of sentences_target: 700000\n",
      "Average number of words in a sentence: 10.360495199292572\n",
      "\n",
      "English sentences 0 to 10:\n",
      "一对 丹顶鹤 正 监视 着 它们 的 筑巢 领地\n",
      "一对 乌鸦 飞 到 我们 屋顶 上 的 巢里 ， 它们 好像 专门 为拉木 而 来 的 。\n",
      "一对 乖乖仔 开着 老爸 的 车子 。\n",
      "一对 九 ？ 一对 九 你 就 全 下注 了 ？\n",
      "一对二 总 不是 好事 ，\n",
      "一对二 ， 沃克 传给 波顿 。\n",
      "一对二 胜 。\n",
      "一对 五 。 一对 五 。 胜算 不大 啊 。\n",
      "一对 五年 没见 过 的 姐妹 一场 激烈 的 争吵 ？\n",
      "一对 五百 诶 。\n",
      "\n",
      "French sentences 0 to 10:\n",
      "A pair of red - crowned cranes have staked out their nesting territory\n",
      "A pair of crows had come to nest on our roof as if they had come for Lhamo.\n",
      "A couple of boys driving around in daddy's car.\n",
      "A pair of nines? You pushed in with a pair of nines?\n",
      "Fighting two against one is never ideal,\n",
      "It's a neat one - two. Walker to Burton.\n",
      "Deuces the winner.\n",
      "Five on one. Five on one. Yeah, not the greatest odds.\n",
      "An incredibly emotional fight between 2 sisters？\n",
      "One against 500.\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "sentences_target = target_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "word_counts_target = [len(sentence.split()) for sentence in sentences_target]\n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Number of sentences_target: {}'.format(len(sentences_target)))\n",
    "\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "outp1 = 'train70.zh.model'\n",
    "outp2 = 'train70.zh.vector'\n",
    "inp = word2vec.LineSentence('train_fenci.zh')\n",
    "model = Word2Vec(inp, size=1000, window=5, min_count=5, workers=4)\n",
    "model.save(outp1)\n",
    "model.wv.save_word2vec_format(outp2, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(u'小姑娘')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老太太 0.8738501667976379\n",
      "小妞 0.8235962390899658\n",
      "娘们 0.811232328414917\n",
      "小丑 0.8110681772232056\n",
      "小孩儿 0.8100305795669556\n",
      "美女 0.8080315589904785\n",
      "辣妹 0.8042834997177124\n",
      "小男孩 0.7993060350418091\n",
      "妞 0.7955087423324585\n",
      "女孩儿 0.7943434119224548\n"
     ]
    }
   ],
   "source": [
    "for each in result:\n",
    "    print (each[0] , each[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list = [u'纽约', u'北京', u'华盛顿', u'女神']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女神\n"
     ]
    }
   ],
   "source": [
    "print (model.doesnt_match(list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
